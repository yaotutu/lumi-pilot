#!/usr/bin/env python3
"""
æµ‹è¯•response_formatå‚æ•°æ”¯æŒ
"""
import asyncio
from infrastructure.llm.client import LLMClient
from infrastructure.mcp.client import MCPManager

async def test_response_format():
    """æµ‹è¯•response_formatå‚æ•°"""
    
    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
    llm_client = LLMClient(debug=True)
    
    print("ğŸ§ª æµ‹è¯•1: æ™®é€šæ–‡æœ¬å“åº”")
    response1 = await llm_client.chat(
        message="ç®€å•ä»‹ç»ä¸€ä¸‹Python",
        max_tokens=100
    )
    print(f"âœ… å“åº”: {response1.message[:100]}...")
    print(f"ğŸ”§ ç”Ÿæˆå‚æ•°: {response1.metadata.get('generation_params', {})}")
    
    print("\nğŸ§ª æµ‹è¯•2: JSONæ ¼å¼å“åº”")
    response2 = await llm_client.chat(
        message="ç”¨JSONæ ¼å¼è¿”å›Pythonçš„ä¸‰ä¸ªä¸»è¦ç‰¹æ€§ï¼Œæ ¼å¼ä¸º: {\"features\": [\"ç‰¹æ€§1\", \"ç‰¹æ€§2\", \"ç‰¹æ€§3\"]}",
        response_format={"type": "json_object"},
        max_tokens=200
    )
    print(f"âœ… å“åº”: {response2.message}")
    print(f"ğŸ”§ ç”Ÿæˆå‚æ•°: {response2.metadata.get('generation_params', {})}")
    
    print("\nğŸ§ª æµ‹è¯•3: é¢å¤–å‚æ•°æ”¯æŒ")
    response3 = await llm_client.chat(
        message="è¯´ä¸€ä¸ªå…³äºç¼–ç¨‹çš„ç¬‘è¯",
        temperature=0.9,
        top_p=0.8,
        stop=["\n\n"],
        max_tokens=100
    )
    print(f"âœ… å“åº”: {response3.message}")
    print(f"ğŸ”§ ç”Ÿæˆå‚æ•°: {response3.metadata.get('generation_params', {})}")

if __name__ == "__main__":
    asyncio.run(test_response_format())