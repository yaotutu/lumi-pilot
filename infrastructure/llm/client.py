"""
ç®€åŒ–çš„å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯
ä½¿ç”¨æ ‡å‡†MCPåè®®ï¼Œé€šè¿‡LangChainè°ƒç”¨OpenAIå…¼å®¹çš„API
"""
import time
import json
from typing import Dict, Any, Optional, List

from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from pydantic import BaseModel

from infrastructure.config.settings import get_settings
from infrastructure.logging.logger import get_logger
from infrastructure.mcp.client import MCPManager

# åˆå§‹åŒ–æ¨¡å—logger
logger = get_logger(__name__)


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ•°æ®æ¨¡å‹"""
    success: bool
    message: str
    data: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    metadata: Dict[str, Any] = {}


class LLMClient:
    """
    å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯
    è´Ÿè´£ä¸LangChainå’ŒOpenAIå…¼å®¹çš„APIè¿›è¡Œäº¤äº’ï¼Œæ”¯æŒMCPå·¥å…·è°ƒç”¨
    """
    
    def __init__(self, mcp_manager: Optional[MCPManager] = None, debug: bool = True):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        self.settings = get_settings()
        self._client: Optional[ChatOpenAI] = None
        self.mcp_manager = mcp_manager
        self.debug = debug
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._init_client()
    
    def _debug_print(self, title: str, data: any):
        """æ‰“å°è°ƒè¯•ä¿¡æ¯"""
        if self.debug:
            print(f"\n{'='*60}")
            print(f"ğŸ” [LLM DEBUG] {title}")
            print(f"{'='*60}")
            if isinstance(data, (dict, list)):
                print(json.dumps(data, ensure_ascii=False, indent=2))
            else:
                print(f"{data}")
            print(f"{'='*60}")
    
    def _init_client(self) -> None:
        """åˆå§‹åŒ–LangChain ChatOpenAIå®¢æˆ·ç«¯"""
        try:
            self._client = ChatOpenAI(
                model=self.settings.openai_model,
                openai_api_key=self.settings.openai_api_key,
                openai_api_base=self.settings.openai_base_url,
                max_tokens=self.settings.max_tokens,
                temperature=self.settings.temperature,
            )
            logger.info("llm_client", f"åˆå§‹åŒ–æˆåŠŸ: {self.settings.openai_model}")
        except Exception as e:
            logger.error("llm_client", f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def chat(
        self, 
        message: str, 
        system_prompt: Optional[str] = None,
        enable_tools: bool = True,
        **kwargs: Any
    ) -> ChatResponse:
        """
        å‘é€èŠå¤©æ¶ˆæ¯å¹¶è·å–å“åº”
        
        Args:
            message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            system_prompt: å¯é€‰çš„ç³»ç»Ÿæç¤ºè¯
            enable_tools: æ˜¯å¦å¯ç”¨MCPå·¥å…·è°ƒç”¨
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆå¦‚model, temperature, max_tokensç­‰ï¼‰
            
        Returns:
            ChatResponse: åŒ…å«å“åº”ç»“æœçš„å¯¹è±¡
        """
        if not self._client:
            return ChatResponse(
                success=False,
                message="",
                error="LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
            )
        
        start_time = time.time()
        current_model = self.settings.openai_model
        
        try:
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¦‚æœæä¾›ï¼‰
            if system_prompt:
                messages.append(SystemMessage(content=system_prompt))
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append(HumanMessage(content=message))
            
            # è·å–MCPå·¥å…·å®šä¹‰
            tools = []
            if enable_tools and self.mcp_manager:
                tools = self.mcp_manager.get_tools_for_llm()
                if tools:
                    logger.info("llm_client", f"å¯ç”¨ {len(tools)} ä¸ªMCPå·¥å…·")
                    for tool in tools:
                        tool_name = tool["function"]["name"]
                        tool_desc = tool["function"]["description"]
                        logger.info("llm_client", f"å¯ç”¨å·¥å…·: {tool_name} - {tool_desc}")
            
            # ä¸´æ—¶è¦†ç›–å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
            temp_client = self._client
            if kwargs:
                client_params = {
                    "model": kwargs.get("model", self.settings.openai_model),
                    "openai_api_key": self.settings.openai_api_key,
                    "openai_api_base": self.settings.openai_base_url,
                    "max_tokens": kwargs.get("max_tokens", self.settings.max_tokens),
                    "temperature": kwargs.get("temperature", self.settings.temperature),
                }
                temp_client = ChatOpenAI(**client_params)
                current_model = client_params["model"]
            
            # è°ƒç”¨API
            logger.info("llm_client", f"è°ƒç”¨API: {current_model}")
            
            # æ‰“å°å‘é€ç»™LLMçš„åŸå§‹æ•°æ®
            self._debug_print("å‘é€ç»™LLMçš„æ¶ˆæ¯", [msg.content if hasattr(msg, 'content') else str(msg) for msg in messages])
            if tools:
                self._debug_print("å‘é€ç»™LLMçš„å·¥å…·å®šä¹‰", tools)
            
            # ç¬¬ä¸€æ¬¡è°ƒç”¨LLM
            if tools:
                response = temp_client.invoke(messages, tools=tools)
            else:
                response = temp_client.invoke(messages)
            
            # æ‰“å°ä»LLMæ¥æ”¶çš„åŸå§‹æ•°æ®
            self._debug_print("ä»LLMæ¥æ”¶çš„å“åº”", {
                "content": response.content if hasattr(response, 'content') else str(response),
                "tool_calls": response.tool_calls if hasattr(response, 'tool_calls') else None,
                "response_type": type(response).__name__
            })
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨è¯·æ±‚
            if tools and hasattr(response, 'tool_calls') and response.tool_calls:
                logger.info("llm_client", f"LLMè¯·æ±‚è°ƒç”¨ {len(response.tool_calls)} ä¸ªå·¥å…·")
                
                # å¤„ç†å·¥å…·è°ƒç”¨
                tool_results = []
                for tool_call in response.tool_calls:
                    result = await self._execute_mcp_tool(tool_call)
                    tool_results.append(result)
                
                # æ„å»ºåŒ…å«å·¥å…·ç»“æœçš„æ–°å¯¹è¯
                conversation = messages.copy()
                conversation.append(response)
                
                # æ·»åŠ å·¥å…·ç»“æœ
                for i, result in enumerate(tool_results):
                    tool_name = response.tool_calls[i].get('name', 'unknown')
                    tool_result_msg = f"å·¥å…· {tool_name} æ‰§è¡Œç»“æœ: {result}"
                    conversation.append(HumanMessage(content=tool_result_msg))
                
                # è®©LLMåŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤
                logger.info("llm_client", "åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤")
                self._debug_print("å‘é€ç»™LLMçš„å®Œæ•´å¯¹è¯ï¼ˆåŒ…å«å·¥å…·ç»“æœï¼‰", [msg.content if hasattr(msg, 'content') else str(msg) for msg in conversation])
                response = temp_client.invoke(conversation)
                
                # æ‰“å°æœ€ç»ˆå“åº”
                self._debug_print("ä»LLMæ¥æ”¶çš„æœ€ç»ˆå“åº”", {
                    "content": response.content if hasattr(response, 'content') else str(response),
                    "response_type": type(response).__name__
                })
            
            # è®¡ç®—è°ƒç”¨æ—¶é•¿
            duration = time.time() - start_time
            
            # æå–å“åº”å†…å®¹
            response_content = response.content if hasattr(response, 'content') else str(response)
            
            # è®°å½•APIè°ƒç”¨æ—¥å¿—
            logger.info("llm_client", f"APIè°ƒç”¨æˆåŠŸ: {current_model}, è€—æ—¶{duration:.2f}s")
            
            # æ„å»ºæˆåŠŸå“åº”
            return ChatResponse(
                success=True,
                message=response_content,
                data={
                    "model": current_model,
                    "input_length": len(message),
                    "response_length": len(response_content),
                    "duration": duration,
                },
                metadata={
                    "timestamp": time.time(),
                    "system_prompt": system_prompt,
                    "tools_used": len(response.tool_calls) if hasattr(response, 'tool_calls') and response.tool_calls else 0,
                    **kwargs
                }
            )
            
        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"LLM APIè°ƒç”¨å¤±è´¥: {str(e)}"
            
            # è®°å½•é”™è¯¯æ—¥å¿—
            logger.error("llm_client", f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
            
            return ChatResponse(
                success=False,
                message="",
                error=error_msg,
                metadata={
                    "timestamp": time.time(),
                    "duration": duration,
                }
            )
    
    async def _execute_mcp_tool(self, tool_call: Dict[str, Any]) -> str:
        """
        æ‰§è¡ŒMCPå·¥å…·è°ƒç”¨ï¼ˆä½¿ç”¨æ ‡å‡†MCPåè®®ï¼‰
        
        Args:
            tool_call: LangChainå·¥å…·è°ƒç”¨å¯¹è±¡
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        try:
            # ä»LangChainå·¥å…·è°ƒç”¨ä¸­æå–ä¿¡æ¯
            tool_name = tool_call.get('name', '')
            tool_args = tool_call.get('args', {})
            
            logger.info("llm_client", f"æ‰§è¡ŒMCPå·¥å…·: {tool_name} å‚æ•°: {tool_args}")
            
            # æ‰“å°MCPå·¥å…·è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯
            self._debug_print("MCPå·¥å…·è°ƒç”¨è¯·æ±‚", {
                "tool_name": tool_name,
                "arguments": tool_args
            })
            
            if not tool_name:
                return "é”™è¯¯: å·¥å…·åç§°ä¸ºç©º"
            
            # é€šè¿‡MCP Managerè°ƒç”¨å·¥å…·
            if self.mcp_manager:
                result = await self.mcp_manager.call_tool(tool_name, tool_args)
                logger.info("llm_client", f"MCPå·¥å…·è°ƒç”¨æˆåŠŸ: {tool_name} ç»“æœ: {result}")
                
                # æ‰“å°MCPå·¥å…·è°ƒç”¨çš„ç»“æœ
                self._debug_print("MCPå·¥å…·è°ƒç”¨ç»“æœ", {
                    "tool_name": tool_name,
                    "result": result
                })
                
                return str(result)
            else:
                return "é”™è¯¯: MCPç®¡ç†å™¨æœªåˆå§‹åŒ–"
                
        except Exception as e:
            error_msg = f"MCPå·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}"
            logger.error("llm_client", error_msg)
            return error_msg
    
    async def validate_connection(self) -> bool:
        """
        éªŒè¯ä¸LLMæœåŠ¡çš„è¿æ¥
        
        Returns:
            bool: è¿æ¥æ˜¯å¦æ­£å¸¸
        """
        try:
            test_response = await self.chat("Hello", max_tokens=10)
            return test_response.success
        except Exception as e:
            logger.error("llm_client", f"è¿æ¥éªŒè¯å¤±è´¥: {str(e)}")
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰æ¨¡å‹ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: æ¨¡å‹é…ç½®ä¿¡æ¯
        """
        mcp_info = {}
        if self.mcp_manager:
            mcp_info = {
                "mcp_enabled": True,
                "mcp_servers": self.mcp_manager.get_server_info()
            }
        else:
            mcp_info = {"mcp_enabled": False}
            
        return {
            "model": self.settings.openai_model,
            "base_url": self.settings.openai_base_url,
            "max_tokens": self.settings.max_tokens,
            "temperature": self.settings.temperature,
            "client_initialized": self._client is not None,
            **mcp_info
        }