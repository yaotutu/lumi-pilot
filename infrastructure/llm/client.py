"""
ç®€åŒ–çš„å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯
ä½¿ç”¨æ ‡å‡†MCPåè®®ï¼Œé€šè¿‡åŸç”ŸHTTPå®¢æˆ·ç«¯è°ƒç”¨OpenAIå…¼å®¹çš„API
"""
import time
import json
import uuid
from typing import Dict, Any, Optional, List

from pydantic import BaseModel

from infrastructure.config.settings import get_settings
from infrastructure.logging.logger import get_logger
from infrastructure.mcp.client import MCPManager
from .openai_client import OpenAIClient, create_system_message, create_user_message, create_assistant_message, create_tool_message

# åˆå§‹åŒ–æ¨¡å—logger
logger = get_logger(__name__)


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ•°æ®æ¨¡å‹"""
    success: bool
    message: str
    data: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    metadata: Dict[str, Any] = {}


class LLMClient:
    """
    å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯
    è´Ÿè´£ä¸OpenAIå…¼å®¹çš„APIè¿›è¡Œäº¤äº’ï¼Œæ”¯æŒMCPå·¥å…·è°ƒç”¨
    """
    
    def __init__(self, mcp_manager: Optional[MCPManager] = None, debug: bool = True):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        self.settings = get_settings()
        self._client: Optional[OpenAIClient] = None
        self.mcp_manager = mcp_manager
        self.debug = debug
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._init_client()
    
    def _debug_print(self, title: str, data: any):
        """æ‰“å°è°ƒè¯•ä¿¡æ¯"""
        if self.debug:
            print(f"\n{'='*60}")
            print(f"ğŸ” [LLM DEBUG] {title}")
            print(f"{'='*60}")
            if isinstance(data, (dict, list)):
                print(json.dumps(data, ensure_ascii=False, indent=2))
            else:
                print(f"{data}")
            print(f"{'='*60}")
    
    def _init_client(self) -> None:
        """åˆå§‹åŒ–OpenAI HTTPå®¢æˆ·ç«¯"""
        try:
            self._client = OpenAIClient(
                api_key=self.settings.openai_api_key,
                base_url=self.settings.openai_base_url,
                model=self.settings.openai_model,
                max_tokens=self.settings.max_tokens,
                temperature=self.settings.temperature,
            )
            logger.info("llm_client", f"åˆå§‹åŒ–æˆåŠŸ: {self.settings.openai_model}")
        except Exception as e:
            logger.error("llm_client", f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def chat(
        self, 
        message: str, 
        system_prompt: Optional[str] = None,
        enable_tools: bool = True,
        **kwargs: Any
    ) -> ChatResponse:
        """
        å‘é€èŠå¤©æ¶ˆæ¯å¹¶è·å–å“åº”
        
        Args:
            message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            system_prompt: å¯é€‰çš„ç³»ç»Ÿæç¤ºè¯
            enable_tools: æ˜¯å¦å¯ç”¨MCPå·¥å…·è°ƒç”¨
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆå¦‚model, temperature, max_tokensç­‰ï¼‰
            
        Returns:
            ChatResponse: åŒ…å«å“åº”ç»“æœçš„å¯¹è±¡
        """
        if not self._client:
            return ChatResponse(
                success=False,
                message="",
                error="LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
            )
        
        start_time = time.time()
        current_model = self.settings.openai_model
        request_id = str(uuid.uuid4())[:8]  # ç”Ÿæˆ8ä½request_id
        tool_start_time = None
        tool_end_time = None
        
        try:
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¦‚æœæä¾›ï¼‰
            if system_prompt:
                messages.append(create_system_message(system_prompt))
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append(create_user_message(message))
            
            # è·å–MCPå·¥å…·å®šä¹‰
            tools = []
            if enable_tools and self.mcp_manager:
                tools = self.mcp_manager.get_tools_for_llm()
                if tools:
                    logger.info("llm_client", f"å¯ç”¨ {len(tools)} ä¸ªMCPå·¥å…·")
                    for tool in tools:
                        tool_name = tool["function"]["name"]
                        tool_desc = tool["function"]["description"]
                        logger.info("llm_client", f"å¯ç”¨å·¥å…·: {tool_name} - {tool_desc}")
            
            # å‡†å¤‡è°ƒç”¨å‚æ•°
            call_params = {
                "model": kwargs.get("model", self.settings.openai_model),
                "max_tokens": kwargs.get("max_tokens", self.settings.max_tokens),
                "temperature": kwargs.get("temperature", self.settings.temperature),
            }
            current_model = call_params["model"]
            
            # è°ƒç”¨API
            logger.info("llm_client", f"è°ƒç”¨API: {current_model}")
            
            # æ‰“å°å‘é€ç»™LLMçš„åŸå§‹æ•°æ®
            self._debug_print("å‘é€ç»™LLMçš„æ¶ˆæ¯", [msg.content if hasattr(msg, 'content') else str(msg) for msg in messages])
            if tools:
                self._debug_print("å‘é€ç»™LLMçš„å·¥å…·å®šä¹‰", tools)
            
            # ç¬¬ä¸€æ¬¡è°ƒç”¨LLM
            response = await self._client.chat_completion(
                messages=messages,
                tools=tools if tools else None,
                **call_params
            )
            
            # æ‰“å°ä»LLMæ¥æ”¶çš„åŸå§‹æ•°æ®
            self._debug_print("ä»LLMæ¥æ”¶çš„å“åº”", {
                "content": response.content if hasattr(response, 'content') else str(response),
                "tool_calls": response.tool_calls if hasattr(response, 'tool_calls') else None,
                "response_type": type(response).__name__
            })
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨è¯·æ±‚
            if tools and hasattr(response, 'tool_calls') and response.tool_calls:
                logger.info("llm_client", f"LLMè¯·æ±‚è°ƒç”¨ {len(response.tool_calls)} ä¸ªå·¥å…·")
                
                # è®°å½•å·¥å…·è°ƒç”¨å¼€å§‹æ—¶é—´
                tool_start_time = time.time()
                
                # å¤„ç†å·¥å…·è°ƒç”¨
                tool_results = []
                for tool_call in response.tool_calls:
                    result = await self._execute_mcp_tool(tool_call)
                    tool_results.append(result)
                
                # è®°å½•å·¥å…·è°ƒç”¨ç»“æŸæ—¶é—´
                tool_end_time = time.time()
                
                # æ„å»ºåŒ…å«å·¥å…·ç»“æœçš„æ–°å¯¹è¯
                conversation = messages.copy()
                # æ·»åŠ åŠ©æ‰‹çš„å·¥å…·è°ƒç”¨å“åº”
                tool_calls_formatted = None
                if response.tool_calls:
                    tool_calls_formatted = [
                        {
                            "id": tc["id"],
                            "type": "function",
                            "function": {
                                "name": tc["name"],
                                "arguments": json.dumps(tc["args"])
                            }
                        }
                        for tc in response.tool_calls
                    ]
                assistant_msg = create_assistant_message(
                    content=response.content if response.content and response.content.strip() else "æ­£åœ¨è°ƒç”¨å·¥å…·...",
                    tool_calls=tool_calls_formatted
                )
                conversation.append(assistant_msg)
                
                # æ·»åŠ å·¥å…·æ‰§è¡Œç»“æœ
                for i, result in enumerate(tool_results):
                    tool_call = response.tool_calls[i]
                    tool_msg = create_tool_message(
                        content=str(result),
                        tool_call_id=tool_call["id"]
                    )
                    conversation.append(tool_msg)
                
                # è®©LLMåŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤
                logger.info("llm_client", "åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤")
                self._debug_print("å‘é€ç»™LLMçš„å®Œæ•´å¯¹è¯ï¼ˆåŒ…å«å·¥å…·ç»“æœï¼‰", [
                    msg.content for msg in conversation
                ])
                response = await self._client.chat_completion(
                    messages=conversation,
                    **call_params
                )
                
                # æ‰“å°æœ€ç»ˆå“åº”
                self._debug_print("ä»LLMæ¥æ”¶çš„æœ€ç»ˆå“åº”", {
                    "content": response.content if hasattr(response, 'content') else str(response),
                    "response_type": type(response).__name__
                })
            
            # è®¡ç®—è°ƒç”¨æ—¶é•¿
            duration = time.time() - start_time
            
            # æå–å“åº”å†…å®¹
            response_content = response.content if hasattr(response, 'content') else str(response)
            
            # è®°å½•APIè°ƒç”¨æ—¥å¿—
            logger.info("llm_client", f"APIè°ƒç”¨æˆåŠŸ: {current_model}, è€—æ—¶{duration:.2f}s")
            
            # æ„å»ºæˆåŠŸå“åº”
            return ChatResponse(
                success=True,
                message=response_content,
                data={
                    "model": current_model,
                    "input_length": len(message),
                    "response_length": len(response_content),
                    "duration": duration,
                },
                metadata={
                    "timestamp": time.time(),
                    "system_prompt": system_prompt,
                    "tools_used": len(response.tool_calls) if hasattr(response, 'tool_calls') and response.tool_calls else 0,
                    # å¯è§‚æµ‹æ€§å­—æ®µ
                    "request_id": request_id,
                    "tool_call_ids": [tc["id"] for tc in response.tool_calls] if hasattr(response, 'tool_calls') and response.tool_calls else [],
                    "llm_latency": duration,
                    "tool_latency": (tool_end_time - tool_start_time) if tool_start_time and tool_end_time else 0,
                    # ç”Ÿæˆå‚æ•°è®°å½•
                    "generation_params": {
                        "model": call_params["model"],
                        "temperature": call_params["temperature"],
                        "max_tokens": call_params["max_tokens"]
                    },
                    **kwargs
                }
            )
            
        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"LLM APIè°ƒç”¨å¤±è´¥: {str(e)}"
            
            # è®°å½•é”™è¯¯æ—¥å¿—
            logger.error("llm_client", f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
            
            return ChatResponse(
                success=False,
                message="",
                error=error_msg,
                metadata={
                    "timestamp": time.time(),
                    "duration": duration,
                    "request_id": request_id,
                    "llm_latency": duration,
                    "tool_latency": 0,
                }
            )
    
    async def _execute_mcp_tool(self, tool_call: Dict[str, Any]) -> str:
        """
        æ‰§è¡ŒMCPå·¥å…·è°ƒç”¨ï¼ˆä½¿ç”¨æ ‡å‡†MCPåè®®ï¼‰
        
        Args:
            tool_call: LangChainå·¥å…·è°ƒç”¨å¯¹è±¡
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        try:
            # ä»å·¥å…·è°ƒç”¨ä¸­æå–ä¿¡æ¯
            tool_name = tool_call.get('name', '')
            tool_args = tool_call.get('args', {})
            
            logger.info("llm_client", f"æ‰§è¡ŒMCPå·¥å…·: {tool_name} å‚æ•°: {tool_args}")
            
            # æ‰“å°MCPå·¥å…·è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯
            self._debug_print("MCPå·¥å…·è°ƒç”¨è¯·æ±‚", {
                "tool_name": tool_name,
                "arguments": tool_args
            })
            
            if not tool_name:
                return "é”™è¯¯: å·¥å…·åç§°ä¸ºç©º"
            
            # é€šè¿‡MCP Managerè°ƒç”¨å·¥å…·
            if self.mcp_manager:
                result = await self.mcp_manager.call_tool(tool_name, tool_args)
                logger.info("llm_client", f"MCPå·¥å…·è°ƒç”¨æˆåŠŸ: {tool_name} ç»“æœ: {result}")
                
                # æ‰“å°MCPå·¥å…·è°ƒç”¨çš„ç»“æœ
                self._debug_print("MCPå·¥å…·è°ƒç”¨ç»“æœ", {
                    "tool_name": tool_name,
                    "result": result
                })
                
                return str(result)
            else:
                return "æŠ±æ­‰ï¼Œå·¥å…·æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
                
        except Exception as e:
            logger.error("llm_client", f"å·¥å…·è°ƒç”¨å¤±è´¥: {tool_name} - {str(e)}")
            # å‹å¥½é™çº§å¤„ç†
            friendly_messages = {
                "printer_status": "æŠ±æ­‰ï¼Œæ‰“å°æœºçŠ¶æ€æŸ¥è¯¢åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•æˆ–æ‰‹åŠ¨æ£€æŸ¥è®¾å¤‡çŠ¶æ€",
                "printer_print": "æŠ±æ­‰ï¼Œæ‰“å°åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ"
            }
            return friendly_messages.get(tool_name, f"æŠ±æ­‰ï¼Œ{tool_name}åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•")
    
    async def validate_connection(self) -> bool:
        """
        éªŒè¯ä¸LLMæœåŠ¡çš„è¿æ¥
        
        Returns:
            bool: è¿æ¥æ˜¯å¦æ­£å¸¸
        """
        try:
            test_response = await self.chat("Hello", max_tokens=10)
            return test_response.success
        except Exception as e:
            logger.error("llm_client", f"è¿æ¥éªŒè¯å¤±è´¥: {str(e)}")
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰æ¨¡å‹ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: æ¨¡å‹é…ç½®ä¿¡æ¯
        """
        mcp_info = {}
        if self.mcp_manager:
            mcp_info = {
                "mcp_enabled": True,
                "mcp_servers": self.mcp_manager.get_server_info()
            }
        else:
            mcp_info = {"mcp_enabled": False}
            
        return {
            "model": self.settings.openai_model,
            "base_url": self.settings.openai_base_url,
            "max_tokens": self.settings.max_tokens,
            "temperature": self.settings.temperature,
            "client_initialized": self._client is not None,
            **mcp_info
        }