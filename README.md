# Lumi Pilot

åŸºäºLangChainçš„AIå¯¹è¯ç³»ç»Ÿï¼Œæ”¯æŒOpenAIå…¼å®¹æ¥å£ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ¤– æ”¯æŒOpenAIå…¼å®¹çš„APIæ¥å£
- ğŸ“¦ ä½¿ç”¨LangChainæ¡†æ¶
- ğŸ”§ å®Œæ•´çš„é…ç½®ç®¡ç†ç³»ç»Ÿ
- ğŸ“Š ç»“æ„åŒ–æ—¥å¿—è®°å½•
- ğŸ–¥ï¸ å‘½ä»¤è¡Œç•Œé¢(CLI)
- ğŸ“„ JSONæ ¼å¼è¾“å‡º

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# ä½¿ç”¨uvå®‰è£…ä¾èµ–
uv sync
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

å°†é…ç½®æ·»åŠ åˆ°ä½ çš„shellé…ç½®æ–‡ä»¶ï¼ˆå¦‚~/.zshrcï¼‰ï¼š

```bash
export LUMI_OPENAI_API_KEY="your_api_key_here"         # å¿…éœ€ï¼šAPIå¯†é’¥
export LUMI_OPENAI_BASE_URL="https://api.openai.com/v1"  # å¯é€‰ï¼šAPIåŸºç¡€URL
export LUMI_OPENAI_MODEL="gpt-3.5-turbo"             # å¯é€‰ï¼šè¦ä½¿ç”¨çš„æ¨¡å‹
export LUMI_TEMPERATURE="0.7"                        # å¯é€‰ï¼šé»˜è®¤æ¸©åº¦å‚æ•°
export LUMI_MAX_TOKENS="1000"                        # å¯é€‰ï¼šé»˜è®¤æœ€å¤§tokenæ•°
```

é‡æ–°åŠ è½½é…ç½®ï¼š
```bash
source ~/.zshrc
```

### 3. éªŒè¯é…ç½®

```bash
# éªŒè¯ç¯å¢ƒé…ç½®
uv run lumi-pilot validate

# æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
uv run lumi-pilot health
```

### 4. å¼€å§‹ä½¿ç”¨

```bash
# ç›´æ¥å¯¹è¯
uv run lumi-pilot "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"

# ä½¿ç”¨chatå‘½ä»¤
uv run lumi-pilot chat "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"

# è‡ªå®šä¹‰å‚æ•°
uv run lumi-pilot chat "å†™ä¸€é¦–è¯—" --temperature 0.9 --max-tokens 200

# ä½¿ç”¨è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
uv run lumi-pilot chat "è§£é‡Šé‡å­è®¡ç®—" --system-prompt "ä½ æ˜¯ä¸€ä½ç‰©ç†å­¦æ•™æˆ"

# ç»„åˆä½¿ç”¨å¤šä¸ªå‚æ•°
uv run lumi-pilot chat "åˆ›ä½œä¸€ä¸ªç§‘å¹»æ•…äº‹" --temperature 0.8 --max-tokens 500
```

## å‘½ä»¤è¡Œé€‰é¡¹

### å…¨å±€é€‰é¡¹

- `--debug`: å¯ç”¨è°ƒè¯•æ¨¡å¼
- `--config`: æ˜¾ç¤ºå½“å‰é…ç½®

### chatå‘½ä»¤é€‰é¡¹

- `--system-prompt, -s`: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
- `--temperature, -t`: æ¸©åº¦å‚æ•° (0.0-2.0)
- `--max-tokens, -m`: æœ€å¤§tokenæ•°
- `--format, -f`: è¾“å‡ºæ ¼å¼ (json/text)

## ç¯å¢ƒå˜é‡

| å˜é‡å | æè¿° | é»˜è®¤å€¼ | ç¤ºä¾‹ |
|--------|------|--------|------|
| `LUMI_OPENAI_API_KEY` | OpenAI APIå¯†é’¥ | **å¿…éœ€** | `sk-...` |
| `LUMI_OPENAI_BASE_URL` | APIåŸºç¡€URL | `https://api.openai.com/v1` | `https://api.deepseek.com/v1` |
| `LUMI_OPENAI_MODEL` | ä½¿ç”¨çš„æ¨¡å‹ | `gpt-3.5-turbo` | `gpt-4`, `deepseek-chat` |
| `LUMI_MAX_TOKENS` | æœ€å¤§tokenæ•° | `1000` | `2000` |
| `LUMI_TEMPERATURE` | æ¸©åº¦å‚æ•° | `0.7` | `0.3`, `0.9` |
| `LUMI_LOG_LEVEL` | æ—¥å¿—çº§åˆ« | `INFO` | `DEBUG`, `WARNING` |
| `LUMI_DEBUG` | è°ƒè¯•æ¨¡å¼ | `false` | `true` |

**æ³¨æ„**: æ¨¡å‹é€‰æ‹©é€šè¿‡ `LUMI_OPENAI_MODEL` ç¯å¢ƒå˜é‡è®¾ç½®ï¼Œæ”¯æŒä»»ä½•OpenAIå…¼å®¹çš„æ¨¡å‹åç§°ã€‚

## è¾“å‡ºæ ¼å¼

é»˜è®¤è¾“å‡ºä¸ºJSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
{
  "status": "success",
  "code": 200,
  "message": "AIå›å¤å†…å®¹",
  "data": {
    "model": "gpt-3.5-turbo",
    "input_length": 10,
    "response_length": 50,
    "duration": 1.23
  },
  "metadata": {
    "app_name": "Lumi Pilot",
    "version": "0.1.0",
    "timestamp": 1234567890
  }
}
```

## é¡¹ç›®ç»“æ„

```
lumi-pilot/
â”œâ”€â”€ lumi_pilot/          # ä¸»è¦ä»£ç 
â”‚   â”œâ”€â”€ cli/            # CLIç›¸å…³
â”‚   â”œâ”€â”€ config/         # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ models/         # å¤§æ¨¡å‹å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ services/       # ä¸šåŠ¡é€»è¾‘
â”‚   â””â”€â”€ utils/          # å·¥å…·å‡½æ•°
â”œâ”€â”€ logs/               # æ—¥å¿—æ–‡ä»¶
â”œâ”€â”€ tests/              # æµ‹è¯•æ–‡ä»¶
â””â”€â”€ pyproject.toml      # é¡¹ç›®é…ç½®
```

## å¼€å‘

### å®‰è£…å¼€å‘ä¾èµ–

```bash
uv sync --dev
```

### ä»£ç æ ¼å¼åŒ–

```bash
uv run black lumi_pilot/
uv run ruff check lumi_pilot/
```

### ç±»å‹æ£€æŸ¥

```bash
uv run mypy lumi_pilot/
```

## åç»­è®¡åˆ’

- [ ] æ·»åŠ gRPCæœåŠ¡å™¨æ”¯æŒ
- [ ] å®ç°MCP (Model Context Protocol) è°ƒç”¨
- [ ] æ·»åŠ å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†
- [ ] æ”¯æŒæµå¼å“åº”
- [ ] æ·»åŠ æ›´å¤šè¾“å‡ºæ ¼å¼